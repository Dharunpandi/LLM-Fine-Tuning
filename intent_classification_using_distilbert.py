# -*- coding: utf-8 -*-
"""intent_Classification_using_distilbert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pZoW2CyV767KQJJ6z_h7fO9LqFCSGevp
"""

!pip install peft
!pip install accelerate
!pip install bitsandBytes

!pip install GPUtil

import torch
import GPUtil
import os

GPUtil.showUtilization()
if torch.cuda.is_available():
  print("Gpu is available")
else:
  device=torch.device("cpu")

os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES']='0'

import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,pipeline,LlamaTokenizer
from huggingface_hub import notebook_login
from datasets import load_dataset
from peft import prepare_model_for_kbit_training,LoraConfig,get_peft_model

if 'COLAB_GPU' in os.environ:
  from google.colab import output
  output.enable_custom_widget_manager()

if 'COLAB_GPU' in os.environ:
  !huggingface-cli login
else:
  notebook_login()

base_model_id="distilbert/distilbert-base-uncased"

bnb_config=BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=6
)

import pandas as pd
from datasets import Dataset
from sklearn.model_selection import train_test_split  # âœ… correct import

# 1. Load your dataset
df = pd.read_csv("/content/ecommerce_intents_50K.csv")

# 2. Create label mapping
label_names = df["label"].unique().tolist()
label2id = {label: i for i, label in enumerate(label_names)}
id2label = {i: label for label, i in label2id.items()}

# 3. Encode labels numerically
df["label"] = df["label"].map(label2id)

# 4. Split into train/test sets
train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)

# 5. Convert pandas DataFrame to Hugging Face Datasets
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

from transformers import AutoTokenizer

tokenizer= AutoTokenizer.from_pretrained("distilbert-base-uncased")
if tokenizer.pad_token is None:
  tokenizer.add_special_tokens({'pad_token':'[PAD]'})

def tokenize(example):
    return tokenizer(example["text"], padding="max_length", truncation=True)

tokenized_train_dataset=train_dataset.map(tokenize,batched=True)
tokenizer_test_dataset=test_dataset.map(tokenize,batched=True)

print(tokenizer.eos_token)

from peft import LoraConfig,get_peft_model,prepare_model_for_kbit_training

model.gradient_checkpointing_enable()

config=LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_lin","v_lin"],
    bias="none",
    lora_dropout=0.05,
    task_type="SEQ_CLS"
)

model=get_peft_model(model,config)
model.print_trainable_parameters()

for name, module in model.named_modules():
    if "lin" in name:
        print(name)

from transformers import Trainer,TrainingArguments,DataCollatorWithPadding

trainer = transformers.Trainer(
    model=model,
    train_dataset=tokenized_train_dataset,
    args=transformers.TrainingArguments(
        output_dir="./finetunedModel",
        per_device_train_batch_size=2,
        gradient_accumulation_steps=2,
        num_train_epochs=3,
        learning_rate=1e-4,
        max_steps=1500,
        bf16=False,
        optim="paged_adamw_8bit",
        logging_dir="./log",
        save_strategy="epoch",
        save_steps=50,
        logging_steps=10

),
    data_collator=DataCollatorWithPadding(tokenizer),
    tokenizer=tokenizer
)
trainer.train()

metrics = trainer.evaluate(eval_dataset=tokenizer_test_dataset)
print("Evaluation metrics:", metrics)



